{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf88f09a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|██████████| 1/1 [00:00<00:00, 165.84it/s]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 165.87it/s]\n",
      "Found cached dataset many_types4_type_script (C:/Users/tltlt/.cache/huggingface/datasets/many_types4_type_script/ManyTypes4TypeScript/1.0.0/f87845becfdb639f5c328d25ec0bba30e959da6024bdbe0575b34d62aa7f188d)\n",
      "100%|██████████| 3/3 [00:01<00:00,  2.87it/s]\n",
      "Downloading: 100%|██████████| 25.0/25.0 [00:00<00:00, 12.3kB/s]\n",
      "c:\\Users\\tltlt\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\huggingface_hub\\file_download.py:123: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\tltlt\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading: 100%|██████████| 899k/899k [00:00<00:00, 1.11MB/s]\n",
      "Downloading: 100%|██████████| 456k/456k [00:00<00:00, 724kB/s]  \n",
      "Downloading: 100%|██████████| 150/150 [00:00<00:00, 50.1kB/s]\n",
      "Downloading: 100%|██████████| 498/498 [00:00<00:00, 477kB/s]\n",
      "Downloading: 100%|██████████| 499M/499M [01:44<00:00, 4.78MB/s] \n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data.dataloader import DataLoader\n",
    "from datasets import load_dataset\n",
    "from transformers import RobertaTokenizerFast, RobertaModel\n",
    "\n",
    "# Uncomment if you want to download the full dataset from hugging face\n",
    "#dataset = load_dataset ( ' kevinjesse /ManyTypes4TypeScript ')\n",
    "\n",
    "#load the small selected local dataset using the py script \n",
    "dataset = load_dataset('ManyTypes4TypeScript.py', ignore_verifications=True)\n",
    "\n",
    "#fast tokenizer for roberta - please stick to the fast one or expect bugs and slowdown\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"microsoft/codebert-base\", add_prefix_space=True)\n",
    "\n",
    "model = RobertaModel.from_pretrained(\"microsoft/codebert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e04b5bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?ba/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1674 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 1/1 [00:04<00:00,  4.57s/ba]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13.15ba/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12.19ba/s]\n"
     ]
    }
   ],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    def divide_chunks(l1, l2, n):\n",
    "        for i in range(0, len(l1), n):\n",
    "            yield {'input_ids': [0] + l1[i:i + n] + [2], 'labels': [-100] + l2[i:i + n] + [-100]}\n",
    "\n",
    "    window_size = 510\n",
    "    tokenized_inputs = tokenizer(examples['tokens'], is_split_into_words=True, truncation=False,\n",
    "                                    add_special_tokens=False)\n",
    "    inputs_ = {'input_ids': [], 'labels': []}\n",
    "\n",
    "    for encoding, label in zip(tokenized_inputs.encodings, examples['labels']):\n",
    "        word_ids = encoding.word_ids  # Map tokens to their respective word.\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:  # Set the special tokens to -100.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
    "                l = label[word_idx] if label[word_idx] is not None else -100\n",
    "                label_ids.append(l)\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        s_labels = set(label_ids)\n",
    "        if len(s_labels) == 1 and list(s_labels)[0] == -100:\n",
    "            continue\n",
    "        for e in divide_chunks(encoding.ids, label_ids, window_size):\n",
    "            for k, v in e.items():\n",
    "                inputs_[k].append(v)\n",
    "\n",
    "    inputs_new = {'input_ids': [], 'm_labels': [], \"masks\": []}\n",
    "\n",
    "    for i in range(len(inputs_['labels'])):\n",
    "        if len(inputs_['input_ids'][i]) != 512:\n",
    "            continue    \n",
    "        for j in range(len(inputs_['labels'][i])):\n",
    "            if inputs_['labels'][i][j]==-100:\n",
    "                continue\n",
    "            copy_label = inputs_['labels'][i].copy()\n",
    "            copy_label[j] = tokenizer.mask_token_id\n",
    "            inputs_new['input_ids'].append(inputs_['input_ids'][i])\n",
    "            inputs_new['m_labels'].append(copy_label)\n",
    "            inputs_new['masks'].append(inputs_['labels'][i][j])\n",
    "    return inputs_new\n",
    "\n",
    "tokenized_hf = dataset.map(tokenize_and_align_labels, batched=True, remove_columns=['id', 'tokens', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c65e6177",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "class TripletLoss(torch.nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "        \n",
    "    def calc_euclidean(self, x1, x2):\n",
    "        return (x1 - x2).pow(2).sum(0)\n",
    "    \n",
    "    def forward(self, anchor: torch.Tensor, positive: torch.Tensor, negative: torch.Tensor) -> torch.Tensor:\n",
    "        distance_positive = self.calc_euclidean(anchor, positive)\n",
    "        distance_negative = self.calc_euclidean(anchor, negative)\n",
    "        losses = torch.relu(distance_positive - distance_negative + self.margin)\n",
    "\n",
    "        return losses.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df96468d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "from typing import Tuple\n",
    "import torch\n",
    "\n",
    "class TripletDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, *in_sequences: torch.Tensor, m_labels: torch.Tensor, labels: torch.Tensor, dataset_name: str,\n",
    "                 train_mode: bool=True):\n",
    "        self.data = TensorDataset(*in_sequences)\n",
    "        self.m_labels = m_labels\n",
    "        self.labels = labels\n",
    "        self.dataset_name = dataset_name\n",
    "        self.train_mode = train_mode\n",
    "\n",
    "        self.get_item_func = self.get_item_train if self.train_mode else self.get_item_test\n",
    "\n",
    "    def get_item_train(self, index: int) -> Tuple[Tuple[torch.Tensor, torch.Tensor],\n",
    "                                         Tuple[torch.Tensor, torch.Tensor], Tuple[torch.Tensor, torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        It returns three tuples. Each one is a (data, label)\n",
    "         - The first tuple is (data, label) at the given index\n",
    "         - The second tuple is similar (data, label) to the given index\n",
    "         - The third tuple is different (data, label) from the given index \n",
    "        \"\"\"\n",
    "\n",
    "         # Find a similar datapoint randomly\n",
    "        mask = self.labels == self.labels[index]\n",
    "        mask[index] = False # Making sure that the similar pair is NOT the same as the given index\n",
    "        mask = mask.nonzero()\n",
    "        a = mask[torch.randint(high=len(mask), size=(1,))][0]\n",
    "\n",
    "        # Find a different datapoint randomly\n",
    "        mask = self.labels != self.labels[index]\n",
    "        mask = mask.nonzero()\n",
    "        b = mask[torch.randint(high=len(mask), size=(1,))][0]\n",
    "        \n",
    "        return (self.data[index], self.m_labels[index]), (self.data[a.item()], self.m_labels[a.item()]), \\\n",
    "               (self.data[b.item()], self.m_labels[b.item()])\n",
    "\n",
    "    def get_item_test(self, index: int) -> Tuple[Tuple[torch.Tensor, torch.Tensor], list, list]:\n",
    "        return (self.data[index], self.labels[index]), [], []\n",
    "    \n",
    "    def __getitem__(self, index: int) -> Tuple[Tuple[torch.Tensor, torch.Tensor],\n",
    "                                         Tuple[torch.Tensor, torch.Tensor], Tuple[torch.Tensor, torch.Tensor]]:\n",
    "         return self.get_item_func(index)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ca6122b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0591211daa8445268bb6bedde842008e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-78.9452,  69.6117,   6.8101, -15.4886,   6.4437, -21.4178, -55.4121,\n",
      "        -59.8180], grad_fn=<AddBackward0>)\n",
      "tensor([192.5033, -42.3269,  25.3756,  -8.2227, -60.8775, -60.0052,  51.7877,\n",
      "        -27.1010], grad_fn=<AddBackward0>)\n",
      "tensor([187.6779,  14.8229, -65.1725,  16.8627,  31.8241, -48.1466, -78.4648,\n",
      "         80.1406], grad_fn=<AddBackward0>)\n",
      "tensor([142.7984, -46.9997,  36.1515,  -2.3187, -36.0093,  12.1763,   6.4032,\n",
      "         12.5049], grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-8-e67abe3bbc2d>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     51\u001B[0m         \u001B[0mloss\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcriterion\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0manchor_out\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mpositive_out\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnegative_out\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     52\u001B[0m         \u001B[0mloss\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 53\u001B[1;33m         \u001B[0moptimizer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32mc:\\Users\\tltlt\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\optim\\optimizer.py\u001B[0m in \u001B[0;36mwrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     87\u001B[0m                 \u001B[0mprofile_name\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;34m\"Optimizer.step#{}.step\"\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mobj\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__class__\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__name__\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     88\u001B[0m                 \u001B[1;32mwith\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mautograd\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mprofiler\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrecord_function\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mprofile_name\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 89\u001B[1;33m                     \u001B[1;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     90\u001B[0m             \u001B[1;32mreturn\u001B[0m \u001B[0mwrapper\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     91\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\Users\\tltlt\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001B[0m in \u001B[0;36mdecorate_context\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     25\u001B[0m         \u001B[1;32mdef\u001B[0m \u001B[0mdecorate_context\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     26\u001B[0m             \u001B[1;32mwith\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__class__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 27\u001B[1;33m                 \u001B[1;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     28\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mcast\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mF\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdecorate_context\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     29\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\Users\\tltlt\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\optim\\adam.py\u001B[0m in \u001B[0;36mstep\u001B[1;34m(self, closure)\u001B[0m\n\u001B[0;32m    106\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    107\u001B[0m             \u001B[0mbeta1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbeta2\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mgroup\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'betas'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 108\u001B[1;33m             F.adam(params_with_grad,\n\u001B[0m\u001B[0;32m    109\u001B[0m                    \u001B[0mgrads\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    110\u001B[0m                    \u001B[0mexp_avgs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\Users\\tltlt\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\optim\\_functional.py\u001B[0m in \u001B[0;36madam\u001B[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001B[0m\n\u001B[0;32m     83\u001B[0m         \u001B[1;31m# Decay the first and second moment running average coefficient\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     84\u001B[0m         \u001B[0mexp_avg\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmul_\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mbeta1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0madd_\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mgrad\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0malpha\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m1\u001B[0m \u001B[1;33m-\u001B[0m \u001B[0mbeta1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 85\u001B[1;33m         \u001B[0mexp_avg_sq\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmul_\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mbeta2\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0maddcmul_\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mgrad\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mgrad\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mvalue\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m1\u001B[0m \u001B[1;33m-\u001B[0m \u001B[0mbeta2\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     86\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mamsgrad\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     87\u001B[0m             \u001B[1;31m# Maintains the maximum of all 2nd moment running avg. till now\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from regex import P\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "epochs = 1\n",
    "\n",
    "class CustomModel(torch.nn.Module):\n",
    "    def __init__(self, model, d, codebert_output_dim = 393216, input_dim = 512): # 50265 + sep + 512 (labels) = 50778\n",
    "        super(CustomModel, self).__init__() \n",
    "        self.d = d\n",
    "        self.model = model\n",
    "        self.config = model.config\n",
    "        self.layer = torch.nn.Linear(codebert_output_dim + input_dim, d)\n",
    "        self.input_dim = input_dim\n",
    "        self.codebert_output_dim = codebert_output_dim\n",
    "    \n",
    "    def forward(self, input_ids=None, attention_mask=None):\n",
    "        \n",
    "        assert input_ids.shape[0] == 1024\n",
    "        \n",
    "        tokens, labels = torch.split(input_ids, self.input_dim)\n",
    "        \n",
    "        model_output = self.model.forward(input_ids=tokens.unsqueeze(0))[0]\n",
    "        \n",
    "        ll_input = torch.cat((model_output.view(1, self.codebert_output_dim).squeeze(0), labels), 0)\n",
    "        assert ll_input.shape[0] == self.codebert_output_dim + self.input_dim\n",
    "        \n",
    "        final_output_tensor = self.layer.forward(ll_input)\n",
    "        \n",
    "        return final_output_tensor\n",
    "\n",
    "custom_model = CustomModel(model, 8)\n",
    "dataset = TripletDataset(torch.tensor(tokenized_hf['train']['input_ids']), m_labels=torch.tensor(tokenized_hf['train']['m_labels']), labels=torch.tensor(tokenized_hf['train']['masks']), dataset_name=\"train\")\n",
    "\n",
    "optimizer = torch.optim.Adam(custom_model.parameters(), lr=0.001)\n",
    "criterion = torch.jit.script(TripletLoss())\n",
    "\n",
    "for epoch in tqdm(range(epochs), desc=\"Epochs\"):\n",
    "    custom_model.train()\n",
    "    running_loss = []\n",
    "    for step in range(len(dataset)):\n",
    "        (t_a, t_p, t_n) = dataset.get_item_func(step)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        anchor_out = custom_model(input_ids=torch.cat((t_a[0][0], t_a[1]), 0))\n",
    "        positive_out = custom_model(input_ids=torch.cat((t_p[0][0], t_p[1]), 0))\n",
    "        negative_out = custom_model(input_ids=torch.cat((t_n[0][0], t_n[1]), 0))\n",
    "        \n",
    "        print(anchor_out)\n",
    "        \n",
    "        loss = criterion(anchor_out[0], positive_out[0], negative_out[0])\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "43dd5c6c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You have to specify either input_ids or inputs_embeds",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [20], line 70\u001B[0m\n\u001B[1;32m     67\u001B[0m     annoy_idx\u001B[39m.\u001B[39mbuild(KNN_TREE_SIZE)\n\u001B[1;32m     68\u001B[0m     \u001B[39mreturn\u001B[39;00m annoy_idx\n\u001B[0;32m---> 70\u001B[0m annoy_idx \u001B[39m=\u001B[39m create_type_space()\n\u001B[1;32m     71\u001B[0m \u001B[39mprint\u001B[39m(annoy_idx)\n",
      "Cell \u001B[0;32mIn [20], line 35\u001B[0m, in \u001B[0;36mcreate_type_space\u001B[0;34m(inputs, labels)\u001B[0m\n\u001B[1;32m     32\u001B[0m tokens_ids\u001B[39m=\u001B[39mtokenizer\u001B[39m.\u001B[39mconvert_tokens_to_ids(tokens)\n\u001B[1;32m     34\u001B[0m \u001B[39m# Get the type space mapping from the model\u001B[39;00m\n\u001B[0;32m---> 35\u001B[0m output \u001B[39m=\u001B[39m custom_model\u001B[39m.\u001B[39;49mforward(torch\u001B[39m.\u001B[39;49mtensor(tokens_ids)[\u001B[39mNone\u001B[39;49;00m,:])\n\u001B[1;32m     37\u001B[0m \u001B[39m# Select masked tokens\u001B[39;00m\n\u001B[1;32m     38\u001B[0m masked_tokens \u001B[39m=\u001B[39m [c \u001B[39mfor\u001B[39;00m c, token \u001B[39min\u001B[39;00m \u001B[39menumerate\u001B[39m(code_tokens) \u001B[39mif\u001B[39;00m token \u001B[39m==\u001B[39m \u001B[39m\"\u001B[39m\u001B[39m<mask>\u001B[39m\u001B[39m\"\u001B[39m]\n",
      "Cell \u001B[0;32mIn [6], line 10\u001B[0m, in \u001B[0;36mCustomModel.forward\u001B[0;34m(self, m_labels, input_ids, attention_mask)\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[39mdef\u001B[39;00m \u001B[39mforward\u001B[39m(\u001B[39mself\u001B[39m, m_labels, input_ids\u001B[39m=\u001B[39m\u001B[39mNone\u001B[39;00m, attention_mask\u001B[39m=\u001B[39m\u001B[39mNone\u001B[39;00m):\n\u001B[0;32m---> 10\u001B[0m     model_output, b \u001B[39m=\u001B[39m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mmodel\u001B[39m.\u001B[39;49mforward(input_ids\u001B[39m=\u001B[39;49minput_ids)    \n\u001B[1;32m     11\u001B[0m     \u001B[39mprint\u001B[39m(m_labels)\n\u001B[1;32m     12\u001B[0m     final_output_tensor \u001B[39m=\u001B[39m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mlayer\u001B[39m.\u001B[39mforward(torch\u001B[39m.\u001B[39mcat((model_output[\u001B[39m0\u001B[39m], torch\u001B[39m.\u001B[39mtensor([tokenizer\u001B[39m.\u001B[39msep_token]), m_labels), \u001B[39m0\u001B[39m))\n",
      "File \u001B[0;32m~/Documents/ml4se/group5/venv/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:804\u001B[0m, in \u001B[0;36mRobertaModel.forward\u001B[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m    802\u001B[0m     input_shape \u001B[39m=\u001B[39m inputs_embeds\u001B[39m.\u001B[39msize()[:\u001B[39m-\u001B[39m\u001B[39m1\u001B[39m]\n\u001B[1;32m    803\u001B[0m \u001B[39melse\u001B[39;00m:\n\u001B[0;32m--> 804\u001B[0m     \u001B[39mraise\u001B[39;00m \u001B[39mValueError\u001B[39;00m(\u001B[39m\"\u001B[39m\u001B[39mYou have to specify either input_ids or inputs_embeds\u001B[39m\u001B[39m\"\u001B[39m)\n\u001B[1;32m    806\u001B[0m batch_size, seq_length \u001B[39m=\u001B[39m input_shape\n\u001B[1;32m    807\u001B[0m device \u001B[39m=\u001B[39m input_ids\u001B[39m.\u001B[39mdevice \u001B[39mif\u001B[39;00m input_ids \u001B[39mis\u001B[39;00m \u001B[39mnot\u001B[39;00m \u001B[39mNone\u001B[39;00m \u001B[39melse\u001B[39;00m inputs_embeds\u001B[39m.\u001B[39mdevice\n",
      "\u001B[0;31mValueError\u001B[0m: You have to specify either input_ids or inputs_embeds"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from annoy import AnnoyIndex\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import time\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "KNN_TREE_SIZE = 20\n",
    "DISTANCE_METRIC = 'euclidean'\n",
    "\n",
    "\n",
    "def create_type_space(inputs=input_list[:4], labels=labels[:4]):\n",
    "    \"\"\"\n",
    "    Creates the type space based on the inputs and their corresponding labels\n",
    "    \"\"\"\n",
    "    \n",
    "    # Make sure imputs are labeled\n",
    "    assert len(inputs) == len(labels)\n",
    "    \n",
    "    # Cache the type space mappings\n",
    "    computed_mapped_batches_train = []\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        # Iterate through the data set\n",
    "        for inp, label in zip(inputs, labels):\n",
    "            \n",
    "            # Tokenize the code\n",
    "            nl_tokens = tokenizer.tokenize(\"\")\n",
    "            code_tokens = tokenizer.tokenize(inp)\n",
    "            tokens=[tokenizer.cls_token]+nl_tokens+[tokenizer.sep_token]+code_tokens+[tokenizer.sep_token]\n",
    "            tokens_ids=tokenizer.convert_tokens_to_ids(tokens)\n",
    "            \n",
    "            # Get the type space mapping from the model\n",
    "            output = custom_model.forward(torch.tensor(tokens_ids)[None,:])\n",
    "            \n",
    "            # Select masked tokens\n",
    "            masked_tokens = [c for c, token in enumerate(code_tokens) if token == \"<mask>\"]\n",
    "            \n",
    "            print(masked_tokens)\n",
    "            \n",
    "            # For this version, assume only one mask\n",
    "            assert len(masked_tokens) == 1\n",
    "            \n",
    "            # Selected only the masked tokens from the output\n",
    "            vals = output.logits.cpu().numpy()\n",
    "            predicted_masks = [vals[0][i] for i in masked_tokens]\n",
    "            \n",
    "            # Cache the mapping of the masked token only\n",
    "            computed_mapped_batches_train.append(predicted_masks)\n",
    "        \n",
    "        # Create the type space\n",
    "        annoy_index = create_knn_index(computed_mapped_batches_train, None, computed_mapped_batches_train[0][0].size)\n",
    "    return annoy_index\n",
    "\n",
    "def create_knn_index(train_types_embed: np.array, valid_types_embed: np.array, type_embed_dim:int) -> AnnoyIndex:\n",
    "    \"\"\"\n",
    "    Creates KNNs index for given type embedding vectors, taken from Type4Py\n",
    "    \"\"\"\n",
    "    \n",
    "    annoy_idx = AnnoyIndex(type_embed_dim, DISTANCE_METRIC)\n",
    "\n",
    "    for i, v in enumerate(tqdm(train_types_embed, total=len(train_types_embed), desc=\"KNN index\")):\n",
    "        print(v[0])\n",
    "        annoy_idx.add_item(i, v[0])\n",
    "\n",
    "    annoy_idx.build(KNN_TREE_SIZE)\n",
    "    return annoy_idx\n",
    "\n",
    "annoy_idx = create_type_space()\n",
    "print(annoy_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "113df0f0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You have to specify either input_ids or inputs_embeds",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [21], line 65\u001B[0m\n\u001B[1;32m     61\u001B[0m             types_score[types_embed_labels[n]] \u001B[39m+\u001B[39m\u001B[39m=\u001B[39m d\n\u001B[1;32m     63\u001B[0m         \u001B[39mreturn\u001B[39;00m \u001B[39msorted\u001B[39m({t: s \u001B[39mfor\u001B[39;00m t, s \u001B[39min\u001B[39;00m types_score\u001B[39m.\u001B[39mitems()}\u001B[39m.\u001B[39mitems(), key\u001B[39m=\u001B[39m\u001B[39mlambda\u001B[39;00m kv: kv[\u001B[39m1\u001B[39m], reverse\u001B[39m=\u001B[39m\u001B[39mTrue\u001B[39;00m)\n\u001B[0;32m---> 65\u001B[0m types_embed_array \u001B[39m=\u001B[39m map_type()\n\u001B[1;32m     66\u001B[0m knn_K \u001B[39m=\u001B[39m \u001B[39m2\u001B[39m\n\u001B[1;32m     67\u001B[0m pred_type_embed, pred_type_score \u001B[39m=\u001B[39m predict_type(types_embed_array, labels[:\u001B[39m4\u001B[39m], annoy_idx, knn_K,)\n",
      "Cell \u001B[0;32mIn [21], line 18\u001B[0m, in \u001B[0;36mmap_type\u001B[0;34m(inputs)\u001B[0m\n\u001B[1;32m     15\u001B[0m tokens_ids\u001B[39m=\u001B[39mtokenizer\u001B[39m.\u001B[39mconvert_tokens_to_ids(tokens)\n\u001B[1;32m     17\u001B[0m \u001B[39m# Get the type space mapping from the model\u001B[39;00m\n\u001B[0;32m---> 18\u001B[0m output \u001B[39m=\u001B[39m custom_model\u001B[39m.\u001B[39;49mforward(torch\u001B[39m.\u001B[39;49mtensor(tokens_ids)[\u001B[39mNone\u001B[39;49;00m,:])\n\u001B[1;32m     20\u001B[0m \u001B[39m# Select masked tokens\u001B[39;00m\n\u001B[1;32m     21\u001B[0m masked_tokens \u001B[39m=\u001B[39m [c \u001B[39mfor\u001B[39;00m c, token \u001B[39min\u001B[39;00m \u001B[39menumerate\u001B[39m(code_tokens) \u001B[39mif\u001B[39;00m token \u001B[39m==\u001B[39m \u001B[39m\"\u001B[39m\u001B[39m<mask>\u001B[39m\u001B[39m\"\u001B[39m]\n",
      "Cell \u001B[0;32mIn [6], line 10\u001B[0m, in \u001B[0;36mCustomModel.forward\u001B[0;34m(self, m_labels, input_ids, attention_mask)\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[39mdef\u001B[39;00m \u001B[39mforward\u001B[39m(\u001B[39mself\u001B[39m, m_labels, input_ids\u001B[39m=\u001B[39m\u001B[39mNone\u001B[39;00m, attention_mask\u001B[39m=\u001B[39m\u001B[39mNone\u001B[39;00m):\n\u001B[0;32m---> 10\u001B[0m     model_output, b \u001B[39m=\u001B[39m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mmodel\u001B[39m.\u001B[39;49mforward(input_ids\u001B[39m=\u001B[39;49minput_ids)    \n\u001B[1;32m     11\u001B[0m     \u001B[39mprint\u001B[39m(m_labels)\n\u001B[1;32m     12\u001B[0m     final_output_tensor \u001B[39m=\u001B[39m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mlayer\u001B[39m.\u001B[39mforward(torch\u001B[39m.\u001B[39mcat((model_output[\u001B[39m0\u001B[39m], torch\u001B[39m.\u001B[39mtensor([tokenizer\u001B[39m.\u001B[39msep_token]), m_labels), \u001B[39m0\u001B[39m))\n",
      "File \u001B[0;32m~/Documents/ml4se/group5/venv/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:804\u001B[0m, in \u001B[0;36mRobertaModel.forward\u001B[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m    802\u001B[0m     input_shape \u001B[39m=\u001B[39m inputs_embeds\u001B[39m.\u001B[39msize()[:\u001B[39m-\u001B[39m\u001B[39m1\u001B[39m]\n\u001B[1;32m    803\u001B[0m \u001B[39melse\u001B[39;00m:\n\u001B[0;32m--> 804\u001B[0m     \u001B[39mraise\u001B[39;00m \u001B[39mValueError\u001B[39;00m(\u001B[39m\"\u001B[39m\u001B[39mYou have to specify either input_ids or inputs_embeds\u001B[39m\u001B[39m\"\u001B[39m)\n\u001B[1;32m    806\u001B[0m batch_size, seq_length \u001B[39m=\u001B[39m input_shape\n\u001B[1;32m    807\u001B[0m device \u001B[39m=\u001B[39m input_ids\u001B[39m.\u001B[39mdevice \u001B[39mif\u001B[39;00m input_ids \u001B[39mis\u001B[39;00m \u001B[39mnot\u001B[39;00m \u001B[39mNone\u001B[39;00m \u001B[39melse\u001B[39;00m inputs_embeds\u001B[39m.\u001B[39mdevice\n",
      "\u001B[0;31mValueError\u001B[0m: You have to specify either input_ids or inputs_embeds"
     ]
    }
   ],
   "source": [
    "def map_type(inputs=input_list[:4]):\n",
    "    \"\"\"\n",
    "    Maps an input to the type space\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        computed_embed_batches_test = []\n",
    "        computed_embed_labels_test = []\n",
    "        \n",
    "        for inp in inputs:\n",
    "\n",
    "            # Tokenize the code\n",
    "            nl_tokens = tokenizer.tokenize(\"\")\n",
    "            code_tokens = tokenizer.tokenize(inp)\n",
    "            tokens=[tokenizer.cls_token]+nl_tokens+[tokenizer.sep_token]+code_tokens+[tokenizer.sep_token]\n",
    "            tokens_ids=tokenizer.convert_tokens_to_ids(tokens)\n",
    "            \n",
    "            # Get the type space mapping from the model\n",
    "            output = custom_model.forward(torch.tensor(tokens_ids)[None,:])\n",
    "            \n",
    "            # Select masked tokens\n",
    "            masked_tokens = [c for c, token in enumerate(code_tokens) if token == \"<mask>\"]\n",
    "            \n",
    "            # For this version, assume only one mask\n",
    "            assert len(masked_tokens) == 1\n",
    "            \n",
    "            # Selected only the masked tokens from the output\n",
    "            vals = output.logits.cpu().numpy()\n",
    "            predicted_masks = [vals[0][i] for i in masked_tokens]\n",
    "\n",
    "            # Cache the mapping of the masked token only\n",
    "            computed_embed_batches_test.append(predicted_masks)\n",
    "        \n",
    "        return computed_embed_batches_test\n",
    "\n",
    "def predict_type(types_embed_array: np.array, types_embed_labels: np.array, indexed_knn: AnnoyIndex, k: int):\n",
    "    \"\"\"\n",
    "    Predict type of given type embedding vectors\n",
    "    \"\"\"\n",
    "\n",
    "    pred_types_embed = []\n",
    "    pred_types_score = []\n",
    "    for i, embed_vec in enumerate(tqdm(types_embed_array, total=len(types_embed_array), desc=\"Finding KNNs & Prediction\")):\n",
    "        \n",
    "        # Get the distances to the KNN\n",
    "        idx, dist = indexed_knn.get_nns_by_vector(embed_vec[0], k, include_distances=True)\n",
    "        \n",
    "        # Compute the scores according to the formula\n",
    "        pred_idx_scores = compute_types_score(dist, idx, types_embed_labels)\n",
    "        \n",
    "        # Cache the scores and the labels\n",
    "        pred_types_embed.append([i for (i, s) in pred_idx_scores])\n",
    "        pred_types_score.append(pred_idx_scores)\n",
    "    \n",
    "    return pred_types_embed, pred_types_score\n",
    "\n",
    "def compute_types_score(types_dist: list, types_idx: list, types_embed_labels: np.array):\n",
    "        types_dist = 1 / (np.array(types_dist) + 1e-10) ** 2\n",
    "        types_dist /= np.sum(types_dist)\n",
    "        types_score = defaultdict(int)\n",
    "        for n, d in zip(types_idx, types_dist):\n",
    "            types_score[types_embed_labels[n]] += d\n",
    "        \n",
    "        return sorted({t: s for t, s in types_score.items()}.items(), key=lambda kv: kv[1], reverse=True)\n",
    "    \n",
    "types_embed_array = map_type()\n",
    "knn_K = 2\n",
    "pred_type_embed, pred_type_score = predict_type(types_embed_array, labels[:4], annoy_idx, knn_K,)\n",
    "print(input_list[:4])\n",
    "print(labels[:4])\n",
    "print(pred_type_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "b898463808f7de9d8eed8bf188ff18ea42a72c5d898f7e9a1365e818ae4a9ee4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}